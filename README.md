# ğŸ“š Rapport de Projet NLP
## SystÃ¨me de Question-RÃ©ponse en Arabe basÃ© sur Wikipedia

---

## ğŸ“‹ Informations GÃ©nÃ©rales

| Ã‰lÃ©ment | DÃ©tail |
|---------|--------|
| **Projet** | SystÃ¨me RAG de Question-RÃ©ponse en Arabe |
| **Auteur** | sonomikane |
| **Date** | Janvier 2026 |
| **ModÃ¨le HuggingFace** | [sonomikane/arabert-qa-arabic-wikipedia](https://huggingface.co/sonomikane/arabert-qa-arabic-wikipedia) |
| **Application Streamlit** | [GitHub Repository](https://github.com/mysonomikane/arabic-qa-streamlit) |

---

## 1. ğŸ¯ Objectif du Projet

DÃ©velopper un **assistant intelligent** capable de rÃ©pondre aux questions en arabe en utilisant Wikipedia arabe comme base de connaissances.

### Architecture RAG (Retrieval-Augmented Generation)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Question      â”‚â”€â”€â”€â–¶â”‚  ğŸ” RETRIEVAL        â”‚â”€â”€â”€â–¶â”‚  ğŸ“š Wikipedia   â”‚
â”‚   utilisateur   â”‚    â”‚  Recherche Wikipedia â”‚    â”‚  Arabe          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Contexte pertinent  â”‚
                       â”‚  (articles trouvÃ©s)  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸ“ RÃ©ponse    â”‚â—€â”€â”€â”€â”‚  ğŸ¤– GENERATION       â”‚
â”‚   extraite      â”‚    â”‚  AraBERT QA Model    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. ğŸ¤– ModÃ¨le de Base

### AraBERT-v2

| CaractÃ©ristique | Valeur |
|-----------------|--------|
| **Nom** | `aubmindlab/bert-base-arabertv2` |
| **Type** | BERT prÃ©-entraÃ®nÃ© pour l'arabe |
| **ParamÃ¨tres** | ~135 millions |
| **Vocabulaire** | 64,000 tokens |
| **CrÃ©ateur** | AUB MIND Lab |

---

## 3. ğŸ“Š Datasets d'EntraÃ®nement

### Datasets UtilisÃ©s

| Dataset | Type | Taille Train | Taille Val |
|---------|------|--------------|------------|
| **TyDi QA Arabic** | Question-RÃ©ponse | ~14,000 | ~1,500 |
| **ARCD** | Arabic Reading Comprehension | ~2,500 | ~700 |
| **XQuAD Arabic** | Cross-lingual QA | - | ~1,100 |

### Total aprÃ¨s preprocessing

| Split | Nombre d'exemples |
|-------|-------------------|
| **Train** | 17,337 |
| **Validation** | 2,963 |

---

## 4. âš™ï¸ Configuration d'EntraÃ®nement

### HyperparamÃ¨tres

| ParamÃ¨tre | Valeur |
|-----------|--------|
| **Epochs** | 3 |
| **Learning Rate** | 3e-5 |
| **Batch Size (Train)** | 16 |
| **Batch Size (Eval)** | 32 |
| **Max Sequence Length** | 384 |
| **Stride** | 128 |
| **Warmup Ratio** | 0.1 |
| **Weight Decay** | 0.01 |
| **FP16** | Oui (Mixed Precision) |

### Environnement

| Ã‰lÃ©ment | SpÃ©cification |
|---------|---------------|
| **Plateforme** | Google Colab |
| **GPU** | Tesla T4 (16 GB) |
| **Temps d'entraÃ®nement** | ~25.5 minutes |

---

## 5. ğŸ“ˆ RÃ©sultats

### MÃ©triques d'Ã‰valuation

| MÃ©trique | Score |
|----------|-------|
| **F1-Score** | **54.36%** |
| **Exact Match (EM)** | **32.80%** |

### InterprÃ©tation

- **F1-Score de 54.36%** : Le modÃ¨le trouve une correspondance partielle correcte dans plus de la moitiÃ© des cas
- **Exact Match de 32.80%** : Le modÃ¨le trouve la rÃ©ponse exacte dans environ 1/3 des cas
- Ces rÃ©sultats sont dans la moyenne pour un modÃ¨le QA en arabe sur des donnÃ©es rÃ©elles

### Comparaison avec l'Ã©tat de l'art

| ModÃ¨le | F1-Score (TyDi QA Arabic) |
|--------|---------------------------|
| mBERT | ~50% |
| XLM-RoBERTa | ~55% |
| **Notre modÃ¨le** | **54.36%** |
| AraELECTRA | ~60% |

---

## 6. ğŸš€ ModÃ¨le Fine-tunÃ©

### Publication sur Hugging Face Hub

| Ã‰lÃ©ment | Valeur |
|---------|--------|
| **Repository** | `sonomikane/arabert-qa-arabic-wikipedia` |
| **URL** | https://huggingface.co/sonomikane/arabert-qa-arabic-wikipedia |
| **TÃ¢che** | Question Answering |
| **Langue** | Arabe (ar) |

### Utilisation

```python
from transformers import pipeline

# Charger le modÃ¨le
qa_pipeline = pipeline(
    "question-answering",
    model="sonomikane/arabert-qa-arabic-wikipedia",
    tokenizer="sonomikane/arabert-qa-arabic-wikipedia"
)

# Poser une question
result = qa_pipeline(
    question="Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…ØµØ±ØŸ",
    context="Ù…ØµØ± Ø¯ÙˆÙ„Ø© Ø¹Ø±Ø¨ÙŠØ© ØªÙ‚Ø¹ ÙÙŠ Ø´Ù…Ø§Ù„ Ø£ÙØ±ÙŠÙ‚ÙŠØ§. Ø¹Ø§ØµÙ…ØªÙ‡Ø§ Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©."
)

print(result)
# {'answer': 'Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©', 'score': 0.95, 'start': 42, 'end': 49}
```

---

## 7. ğŸŒ Application Streamlit

### FonctionnalitÃ©s

1. **Recherche automatique** dans Wikipedia arabe (composant RETRIEVAL)
2. **Extraction de rÃ©ponses** avec le modÃ¨le fine-tunÃ© (composant GENERATION)
3. **Interface bilingue** arabe/franÃ§ais
4. **Affichage des sources** Wikipedia
5. **Score de confiance** pour chaque rÃ©ponse

### DÃ©ploiement

| Plateforme | URL |
|------------|-----|
| **GitHub** | https://github.com/mysonomikane/arabic-qa-streamlit |
| **Streamlit Cloud** | DÃ©ploiement automatique depuis GitHub |

### Technologies UtilisÃ©es

| Technologie | Version | Usage |
|-------------|---------|-------|
| Streamlit | 1.29.0 | Interface web |
| Transformers | 4.36.0 | ModÃ¨le QA |
| PyTorch | Latest | Backend ML |
| Requests | Latest | API Wikipedia |

### Architecture de l'Application

```
streamlit_app/
â”œâ”€â”€ app.py              # Application principale
â”œâ”€â”€ requirements.txt    # DÃ©pendances
â””â”€â”€ .streamlit/
    â””â”€â”€ config.toml     # Configuration Streamlit
```

---

## 8. ğŸ“ Structure du Projet

```
nlp/
â”œâ”€â”€ projet_NLP_WIKI.ipynb      # Notebook d'entraÃ®nement (Colab)
â”œâ”€â”€ test_model.py              # Script de test
â”œâ”€â”€ RAPPORT_PROJET_NLP.md      # Ce rapport
â””â”€â”€ streamlit_app/             # Application web
    â”œâ”€â”€ app.py
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ .streamlit/
        â””â”€â”€ config.toml
```

---

## 9. ğŸ”„ Pipeline Complet

### Ã‰tape 1 : PrÃ©paration des donnÃ©es
- Chargement de TyDi QA, ARCD, XQuAD
- Filtrage des exemples avec rÃ©ponses
- Tokenization avec AraBERT tokenizer
- Gestion des positions start/end

### Ã‰tape 2 : Fine-tuning
- Chargement du modÃ¨le AraBERT-v2
- EntraÃ®nement sur GPU (Tesla T4)
- 3 epochs avec early stopping
- Sauvegarde du meilleur modÃ¨le

### Ã‰tape 3 : Ã‰valuation
- Calcul du F1-Score et Exact Match
- Validation sur 3 datasets

### Ã‰tape 4 : Publication
- Upload sur Hugging Face Hub
- Documentation du modÃ¨le

### Ã‰tape 5 : DÃ©ploiement
- CrÃ©ation de l'application Streamlit
- IntÃ©gration avec Wikipedia API
- DÃ©ploiement sur Streamlit Cloud

---

## 10. ğŸ’» Explication DÃ©taillÃ©e du Code

### Cellule 1 : VÃ©rification de l'environnement GPU

```python
import torch
if torch.cuda.is_available():
    print(f"GPU DÃ‰TECTÃ‰ : {torch.cuda.get_device_name(0)}")
```

**Objectif :** VÃ©rifier que Google Colab dispose d'un GPU (Tesla T4) pour accÃ©lÃ©rer l'entraÃ®nement. Sans GPU, l'entraÃ®nement prendrait plusieurs heures au lieu de ~25 minutes.

---

### Cellule 2 : Installation des dÃ©pendances

```python
!pip install -q transformers==4.36.0 datasets==2.16.0
!pip install -q arabert faiss-cpu sentence-transformers
```

**Packages installÃ©s :**

| Package | UtilitÃ© |
|---------|---------|
| `transformers` | BibliothÃ¨que Hugging Face pour les modÃ¨les NLP |
| `datasets` | Chargement des datasets (TyDi QA, ARCD, XQuAD) |
| `arabert` | PrÃ©processeur spÃ©cifique pour le texte arabe |
| `faiss-cpu` | Recherche vectorielle rapide (systÃ¨me RAG) |
| `sentence-transformers` | CrÃ©ation d'embeddings pour la recherche |

---

### Cellule 3 : Chargement des Datasets

```python
# TyDi QA Arabic (~14,000 exemples train)
tydiqa = load_dataset("tydiqa", "primary_task")
tydiqa_train = tydiqa['train'].filter(lambda x: x['language'] == 'arabic')

# Arabic SQuAD / ARCD (~2,500 exemples)
arabic_squad = load_dataset("arcd")

# XQuAD Arabic (validation uniquement)
xquad = load_dataset("xquad", "xquad.ar")
```

**Explication :** On charge trois datasets de Question-RÃ©ponse en arabe et on les combine pour avoir plus de donnÃ©es d'entraÃ®nement (17,337 exemples au total).

---

### Cellule 4 : Preprocessing des donnÃ©es

#### 4.1. Filtrage des exemples avec rÃ©ponses

```python
def has_answer_tydiqa(example):
    start_bytes = example['annotations']['minimal_answers_start_byte']
    end_bytes = example['annotations']['minimal_answers_end_byte']
    return start_bytes[0] != -1 and end_bytes[0] != -1
```

**Explication :** On garde uniquement les exemples qui ont une rÃ©ponse valide (position de dÃ©but et fin != -1).

#### 4.2. Tokenization avec AraBERT

```python
tokenized = tokenizer(
    questions, 
    contexts, 
    truncation="only_second",  # Tronquer le contexte si trop long
    max_length=384,            # Longueur max en tokens
    stride=128,                # Chevauchement pour les longs textes
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
    padding="max_length"
)
```

**ParamÃ¨tres clÃ©s :**
- `max_length=384` : Le modÃ¨le BERT a une limite de 512 tokens, on utilise 384 pour la sÃ©curitÃ©
- `stride=128` : Si le texte est trop long, on crÃ©e des fenÃªtres qui se chevauchent de 128 tokens
- `return_offsets_mapping` : Permet de mapper les positions de caractÃ¨res aux positions de tokens

#### 4.3. Calcul des positions start/end

```python
# Conversion bytes â†’ caractÃ¨res (TyDi QA utilise des bytes)
context_bytes = context.encode('utf-8')
prefix = context_bytes[:start_byte].decode('utf-8')
start_char = len(prefix)
end_char = start_char + len(answer)

# Trouver les tokens correspondants
token_start = context_start
while token_start <= context_end and offsets[token_start][0] <= start_char:
    token_start += 1
token_start -= 1
```

**Explication :** Le modÃ¨le QA prÃ©dit la position du premier et dernier token de la rÃ©ponse. On doit donc convertir les positions en caractÃ¨res vers des positions en tokens.

---

### Cellule 5 : Fine-tuning du modÃ¨le

#### 5.1. Chargement du modÃ¨le AraBERT

```python
model = AutoModelForQuestionAnswering.from_pretrained("aubmindlab/bert-base-arabertv2")
model = model.cuda()  # DÃ©placer sur GPU
```

**AraBERT-v2 :** ModÃ¨le BERT prÃ©-entraÃ®nÃ© sur 77GB de texte arabe (Wikipedia, journaux, livres).

#### 5.2. Configuration de l'entraÃ®nement

```python
training_args = TrainingArguments(
    learning_rate=3e-5,      # Taux d'apprentissage standard pour fine-tuning BERT
    num_train_epochs=3,      # 3 passages sur les donnÃ©es
    per_device_train_batch_size=16,
    warmup_ratio=0.1,        # 10% des steps pour augmenter progressivement le LR
    fp16=True,               # Mixed precision pour accÃ©lÃ©rer sur GPU
    weight_decay=0.01,       # RÃ©gularisation L2
)
```

**Justification des hyperparamÃ¨tres :**

| ParamÃ¨tre | Valeur | Justification |
|-----------|--------|---------------|
| `learning_rate` | 3e-5 | Valeur standard pour fine-tuning BERT (recommandÃ©: 2e-5 Ã  5e-5) |
| `epochs` | 3 | Suffisant pour un rÃ©sultat moyen, Ã©vite le surapprentissage |
| `batch_size` | 16 | Compromis mÃ©moire GPU / vitesse |
| `fp16` | True | Divise par 2 l'utilisation mÃ©moire, accÃ©lÃ¨re x2 |
| `warmup_ratio` | 0.1 | Stabilise l'entraÃ®nement au dÃ©but |

#### 5.3. MÃ©triques d'Ã©valuation

```python
def compute_metrics(pred):
    # Exact Match : rÃ©ponse parfaitement correcte
    em = np.mean((start_pred == start_true) & (end_pred == end_true))
    
    # F1-Score : chevauchement partiel entre prÃ©diction et vÃ©ritÃ©
    precision = len(pred_tokens & true_tokens) / len(pred_tokens)
    recall = len(pred_tokens & true_tokens) / len(true_tokens)
    f1 = 2 * precision * recall / (precision + recall)
```

**Deux mÃ©triques :**
- **Exact Match (EM)** : 1 si la rÃ©ponse est exactement correcte, 0 sinon
- **F1-Score** : Mesure le chevauchement entre la rÃ©ponse prÃ©dite et la vraie rÃ©ponse

---

### Cellule 6 : Publication sur Hugging Face Hub

```python
from huggingface_hub import login
login()  # Connexion avec token

model.push_to_hub("sonomikane/arabert-qa-arabic-wikipedia")
tokenizer.push_to_hub("sonomikane/arabert-qa-arabic-wikipedia")
```

**RÃ©sultat :** Le modÃ¨le est accessible publiquement sur https://huggingface.co/sonomikane/arabert-qa-arabic-wikipedia

---

### Cellule 7 : SystÃ¨me RAG avec Wikipedia

#### 7.1. TÃ©lÃ©chargement Wikipedia arabe

```python
wiki_dataset = load_dataset("wikimedia/wikipedia", "20231101.ar", split="train")
# ~600,000 articles
```

#### 7.2. CrÃ©ation des embeddings

```python
encoder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
embeddings = encoder.encode(paragraph_texts, batch_size=256)
```

**Sentence-Transformers :** Convertit chaque paragraphe en un vecteur de 384 dimensions qui capture le sens sÃ©mantique.

#### 7.3. Index FAISS

```python
index = faiss.IndexFlatIP(dimension)  # Inner Product = Cosine similarity
faiss.normalize_L2(embeddings)
index.add(embeddings)
```

**FAISS :** BibliothÃ¨que Facebook pour la recherche rapide de vecteurs similaires. Permet de trouver les paragraphes les plus pertinents en millisecondes.

---

### Cellule 8 : Application Streamlit

```python
def answer_from_wikipedia(question, num_results=3):
    # 1. RETRIEVAL : Chercher les paragraphes pertinents
    query_embedding = encoder.encode([question])
    scores, indices = index.search(query_embedding, num_results)
    
    # 2. Combiner les contextes
    combined_context = " ".join([paragraphs[i]['text'] for i in indices[0]])
    
    # 3. GENERATION : Extraire la rÃ©ponse avec AraBERT
    result = qa_pipeline(question=question, context=combined_context)
    return result['answer'], result['score']
```

**Architecture RAG :**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Question   â”‚â”€â”€â”€â”€â–¶â”‚  1. RETRIEVAL    â”‚â”€â”€â”€â”€â–¶â”‚  Paragraphes    â”‚
â”‚  utilisateurâ”‚     â”‚  (FAISS search)  â”‚     â”‚  Wikipedia      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Contexte        â”‚
                    â”‚  combinÃ©         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RÃ©ponse    â”‚â—€â”€â”€â”€â”€â”‚  2. GENERATION   â”‚
â”‚  extraite   â”‚     â”‚  (AraBERT QA)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 11. ğŸ“ Conclusion

### Objectifs Atteints

âœ… ModÃ¨le QA fine-tunÃ© sur donnÃ©es arabes  
âœ… F1-Score de 54.36% (objectif "rÃ©sultat moyen" atteint)  
âœ… Publication sur Hugging Face Hub  
âœ… Application web avec recherche Wikipedia  
âœ… SystÃ¨me RAG fonctionnel  

### AmÃ©liorations Possibles

1. **Plus d'epochs** : EntraÃ®ner sur 5-10 epochs pour de meilleurs rÃ©sultats
2. **Data Augmentation** : Ajouter plus de donnÃ©es d'entraÃ®nement
3. **ModÃ¨le plus grand** : Utiliser AraBERT-large au lieu de base
4. **Index local** : CrÃ©er un index FAISS de Wikipedia pour une recherche plus rapide
5. **Caching** : Mettre en cache les rÃ©sultats Wikipedia

---

## 12. ğŸ“š RÃ©fÃ©rences

- [AraBERT Paper](https://arxiv.org/abs/2003.00104)
- [TyDi QA Dataset](https://ai.google.com/research/tydiqa)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [Streamlit Documentation](https://docs.streamlit.io)
- [Wikipedia API](https://www.mediawiki.org/wiki/API:Main_page)

---

## 13. ğŸ”— Liens du Projet

| Ressource | Lien |
|-----------|------|
| **ModÃ¨le HuggingFace** | https://huggingface.co/sonomikane/arabert-qa-arabic-wikipedia |
| **Code GitHub** | https://github.com/mysonomikane/arabic-qa-streamlit |
| **Application Live** | Streamlit Cloud (auto-dÃ©ployÃ©e) |
| **AraBERT Original** | https://huggingface.co/aubmindlab/bert-base-arabertv2 |

---

*Rapport gÃ©nÃ©rÃ© le 12 Janvier 2026*
