import streamlit as st
from transformers import pipeline
import requests
import time
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# Configuration de la page
st.set_page_config(
    page_title="Ù…Ø³Ø§Ø¹Ø¯ ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
    page_icon="ğŸ”",
    layout="centered"
)

# CSS pour le support RTL (arabe)
st.markdown("""
<style>
    .stTextInput input {
        direction: rtl;
        text-align: right;
        font-size: 20px;
        padding: 15px;
    }
    .answer-box {
        background: linear-gradient(135deg, #1e88e5 0%, #1565c0 100%);
        padding: 25px;
        border-radius: 15px;
        color: white;
        direction: rtl;
        text-align: right;
        margin: 20px 0;
    }
    .context-box {
        background-color: #f5f5f5;
        padding: 15px;
        border-radius: 10px;
        direction: rtl;
        text-align: right;
        margin: 10px 0;
        border-left: 4px solid #1e88e5;
    }
    .source-link {
        background-color: #e3f2fd;
        padding: 8px 15px;
        border-radius: 20px;
        margin: 5px;
        display: inline-block;
    }
</style>
""", unsafe_allow_html=True)

# === Charger le modÃ¨le depuis Hugging Face ===
@st.cache_resource
def load_model():
    """Charge le modÃ¨le QA fine-tunÃ© depuis Hugging Face"""
    return pipeline(
        "question-answering",
        model="sonomikane/arabert-qa-arabic-wikipedia",
        tokenizer="sonomikane/arabert-qa-arabic-wikipedia",
        device=-1
    )

# === Session requÃªtes robuste pour Wikipedia ===
def get_wikipedia_session():
    """CrÃ©e une session robuste pour accÃ©der Ã  Wikipedia API"""
    session = requests.Session()
    
    # StratÃ©gie de retry robuste
    retry_strategy = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET"]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    # Headers complets
    session.headers.update({
        "User-Agent": "ArabicQABot/1.0 (Arabic Wikipedia QA; +https://github.com/mysonomikane/arabic-qa-streamlit)",
        "Accept-Language": "ar,en-US;q=0.9",
        "Accept": "application/json",
        "Accept-Encoding": "gzip, deflate"
    })
    
    return session

# === Recherche Wikipedia Arabe ===
def search_wikipedia_arabic(query, num_results=5):
    """
    Recherche dans Wikipedia arabe et retourne le contenu des articles pertinents.
    C'est le composant RETRIEVAL du systÃ¨me RAG.
    """
    try:
        session = get_wikipedia_session()
        api_url = "https://ar.wikipedia.org/w/api.php"
        
        # Ã‰tape 1: Rechercher les articles pertinents
        search_params = {
            "action": "query",
            "list": "search",
            "srsearch": query,
            "srlimit": num_results,
            "format": "json"
        }
        
        response = session.get(api_url, params=search_params, timeout=20)
        response.raise_for_status()
        search_data = response.json()
        
        if "query" not in search_data or not search_data["query"]["search"]:
            return None, [], "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬"
        
        # RÃ©cupÃ©rer les titres des articles trouvÃ©s
        titles = [result["title"] for result in search_data["query"]["search"]]
        
        # DÃ©lai pour respecter le rate limiting de Wikipedia
        time.sleep(0.5)
        
        # Ã‰tape 2: RÃ©cupÃ©rer le contenu des articles
        content_params = {
            "action": "query",
            "titles": "|".join(titles[:3]),  # Limiter Ã  3 articles
            "prop": "extracts",
            "exintro": False,
            "explaintext": True,
            "exlimit": 3,
            "format": "json"
        }
        
        response = session.get(api_url, params=content_params, timeout=20)
        response.raise_for_status()
        content_data = response.json()
        
        # Extraire le contenu
        pages = content_data.get("query", {}).get("pages", {})
        contexts = []
        sources = []
        
        for page_id, page in pages.items():
            if page_id != "-1" and "extract" in page:
                text = page["extract"]
                # Prendre les premiers 1500 caractÃ¨res de chaque article
                if len(text) > 100:
                    contexts.append(text[:1500])
                    sources.append({
                        "title": page.get("title", ""),
                        "url": f"https://ar.wikipedia.org/wiki/{page.get('title', '').replace(' ', '_')}"
                    })
        
        if not contexts:
            return None, [], "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰"
        
        # Combiner tous les contextes
        combined_context = "\n\n".join(contexts)
        return combined_context, sources, None
        
    except requests.exceptions.Timeout:
        return None, [], "â±ï¸ Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ (Timeout). Ø­Ø§ÙˆÙ„ Ù„Ø§Ø­Ù‚Ø§Ù‹."
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 403:
            return None, [], "ğŸš« Ø±ÙØ¶Øª ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ Ø§Ù„Ø·Ù„Ø¨ (403). Ø¬Ø±Ø¨ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¢Ø®Ø±."
        elif e.response.status_code == 429:
            return None, [], "â³ Ø·Ù„Ø¨Ø§Øª ÙƒØ«ÙŠØ±Ø©. Ø§Ù†ØªØ¸Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ø«Ù… Ø­Ø§ÙˆÙ„ Ù…Ø¬Ø¯Ø¯Ø§Ù‹."
        else:
            return None, [], f"âŒ Ø®Ø·Ø£ HTTP: {e.response.status_code}"
    except requests.exceptions.ConnectionError:
        return None, [], "âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§. ØªØ­Ù‚Ù‚ Ù…Ù† Ø§ØªØµØ§Ù„Ùƒ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª."
    except Exception as e:
        return None, [], f"âŒ Ø®Ø·Ø£: {str(e)}"

# === Interface principale ===
st.markdown("""
# ğŸ” Ù…Ø³Ø§Ø¹Ø¯ ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
## Arabic Wikipedia Assistant

**Ø§Ø³Ø£Ù„Ù†ÙŠ Ø£ÙŠ Ø³Ø¤Ø§Ù„ ÙˆØ³Ø£Ø¨Ø­Ø« ÙÙŠ ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ø£Ø¬Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©!**

ğŸ¤– Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠØ³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ AraBERT Ø§Ù„Ù…ÙØ¯Ø±ÙÙ‘Ø¨ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
""")

st.divider()

# Charger le modÃ¨le
with st.spinner("â³ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬... (Chargement du modÃ¨le)"):
    qa_pipeline = load_model()

# Zone de question principale
question = st.text_input(
    "â“ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§:",
    placeholder="Ù…Ø«Ø§Ù„: Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…ØµØ±ØŸ Ù…Ù† Ù‡Ùˆ Ø·Ù‡ Ø­Ø³ÙŠÙ†ØŸ Ù…ØªÙ‰ ØªØ£Ø³Ø³Øª Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©ØŸ",
    key="main_question"
)

# Exemples de questions
st.markdown("### ğŸ’¡ Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:")
col1, col2, col3 = st.columns(3)

example_questions = [
    "Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…ØµØ±ØŸ",
    "Ù…Ù† Ù‡Ùˆ Ù†Ø¬ÙŠØ¨ Ù…Ø­ÙÙˆØ¸ØŸ",
    "Ù…Ø§ Ù‡Ùˆ Ù†Ù‡Ø± Ø§Ù„Ù†ÙŠÙ„ØŸ",
    "Ù…ØªÙ‰ ØªØ£Ø³Ø³Øª Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©ØŸ",
    "Ù…Ù† Ù‡Ùˆ ØµÙ„Ø§Ø­ Ø§Ù„Ø¯ÙŠÙ† Ø§Ù„Ø£ÙŠÙˆØ¨ÙŠØŸ",
    "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø£Ù‡Ø±Ø§Ù…Ø§ØªØŸ"
]

cols = st.columns(3)
for i, q in enumerate(example_questions):
    with cols[i % 3]:
        if st.button(q, key=f"ex_{i}", use_container_width=True):
            st.session_state.main_question = q
            st.rerun()

st.divider()

# Bouton de recherche
search_clicked = st.button(
    "ğŸ” Ø§Ø¨Ø­Ø« ÙÙŠ ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§",
    type="primary",
    use_container_width=True
)

# === Traitement de la question ===
if search_clicked and question:
    
    # Ã‰tape 1: Recherche dans Wikipedia (RETRIEVAL)
    with st.spinner("ğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©..."):
        context, sources, error = search_wikipedia_arabic(question)
    
    if error:
        st.error(f"âŒ {error}")
    elif context:
        # Afficher les sources trouvÃ©es
        st.success(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(sources)} Ù…Ù‚Ø§Ù„Ø§Øª Ø°Ø§Øª ØµÙ„Ø©")
        
        # Ã‰tape 2: Extraction de la rÃ©ponse (GENERATION)
        with st.spinner("ğŸ¤” Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©..."):
            try:
                result = qa_pipeline(
                    question=question,
                    context=context,
                    max_answer_len=150
                )
                
                answer = result["answer"]
                score = result["score"]
                
                # Afficher la rÃ©ponse
                st.markdown(f"""
                <div class="answer-box">
                    <p style="font-size: 16px; opacity: 0.9;">â“ Ø§Ù„Ø³Ø¤Ø§Ù„: {question}</p>
                    <h2 style="font-size: 28px; margin: 15px 0;">ğŸ“ {answer}</h2>
                    <p style="font-size: 14px;">ğŸ¯ Ù†Ø³Ø¨Ø© Ø§Ù„Ø«Ù‚Ø©: {score*100:.1f}%</p>
                </div>
                """, unsafe_allow_html=True)
                
                # Barre de confiance
                st.progress(score)
                
                # Avertissement si confiance faible
                if score < 0.3:
                    st.warning("âš ï¸ Ù†Ø³Ø¨Ø© Ø§Ù„Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø©. Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ù„Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„.")
                
                # Afficher les sources
                st.markdown("### ğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± Ù…Ù† ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§:")
                for src in sources:
                    st.markdown(f'<span class="source-link">ğŸ“„ <a href="{src["url"]}" target="_blank">{src["title"]}</a></span>', unsafe_allow_html=True)
                
                # Afficher le contexte utilisÃ©
                with st.expander("ğŸ“– Ø¹Ø±Ø¶ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù…Ù† ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§"):
                    st.markdown(f'<div class="context-box">{context[:2000]}...</div>', unsafe_allow_html=True)
                    
            except Exception as e:
                st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {str(e)}")
    else:
        st.warning("âš ï¸ Ù„Ù… Ø£Ø¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ©. Ø¬Ø±Ø¨ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¢Ø®Ø± Ø£Ùˆ Ø£Ø¹Ø¯ ØµÙŠØ§ØºØªÙ‡.")

elif search_clicked:
    st.warning("âš ï¸ Ø§Ù„Ø±Ø¬Ø§Ø¡ ÙƒØªØ§Ø¨Ø© Ø³Ø¤Ø§Ù„ Ø£ÙˆÙ„Ø§Ù‹")

# === Sidebar avec informations ===
with st.sidebar:
    st.markdown("## â„¹ï¸ Ø¹Ù† Ø§Ù„Ù†Ø¸Ø§Ù…")
    st.markdown("""
    **ğŸ¤– ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ Ø§Ù„Ù†Ø¸Ø§Ù… (RAG):**
    
    1ï¸âƒ£ **Ø§Ù„Ø¨Ø­Ø« (Retrieval)**
    - ÙŠØ¨Ø­Ø« ÙÙŠ ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    - ÙŠØ¬Ù„Ø¨ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©
    
    2ï¸âƒ£ **Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ (Generation)**
    - ÙŠØ­Ù„Ù„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙØ³ØªØ±Ø¬Ø¹Ø©
    - ÙŠØ³ØªØ®Ø±Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©
    
    ---
    
    **ğŸ“Š Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:**
    - **Ø§Ù„Ø§Ø³Ù…:** AraBERT-QA
    - **Ø§Ù„ØªØ¯Ø±ÙŠØ¨:** TyDi QA + ARCD + XQuAD
    - **F1-Score:** 54.36%
    - **Exact Match:** 32.80%
    
    ---
    
    **ğŸ”— Ø§Ù„Ø±ÙˆØ§Ø¨Ø·:**
    """)
    
    st.markdown("[ğŸ¤— Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ HuggingFace](https://huggingface.co/sonomikane/arabert-qa-arabic-wikipedia)")
    st.markdown("[ğŸ“– ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](https://ar.wikipedia.org)")

# Footer
st.divider()
st.markdown("""
<div style="text-align: center; color: gray; font-size: 12px;">
    ğŸ” <strong>Arabic Wikipedia QA Assistant</strong> | 
    Ù†Ø¸Ø§Ù… RAG Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù…Ù† ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©<br>
    <a href="https://huggingface.co/sonomikane/arabert-qa-arabic-wikipedia">sonomikane/arabert-qa-arabic-wikipedia</a>
</div>
""", unsafe_allow_html=True)
